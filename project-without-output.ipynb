{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f8ffa2",
   "metadata": {},
   "source": [
    "# Enhancing Fake News Detection Accuracy and Interpretability: Combining Historical Credibility Scores with Explainable AI\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook implements the core components of a fake news detection system, focusing on enhancing detection accuracy and interpretability by integrating historical credibility scores with Explainable AI techniques. The steps include loading the enhanced LIAR-PLUS dataset, preprocessing, feature extraction, model development, and interpretability analysis using SHAP. By emphasizing both accuracy and explainability, the system aims to provide not only reliable predictions but also transparent insights into the modle's decision-making process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7482ed7",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Dataset Loading and Preprocessing\n",
    "\n",
    "### Dataset Loading\n",
    "\n",
    "The LIAR-PLUS dataset [Alhindi et al., 2018], an extension of the LIAR dataset [Wang, 2017], comprises 12.8K manually labeled short statements from politifact.com, annotated for truthfulness. This dataset includes statement content, speaker details, context, and credibility labels, along with justifications provided by human fact-checkers. These justifications offer valuable context, enabling the development of models that are both accurate and explainable.\n",
    "\n",
    "First, we load three distinct datasets—training, testing, and validation—each playing a critical role in developing and evaluating our model. After loading, we organize the data into pandas DataFrames to make it easier to work with later. This step prepares the data for the upcoming stages, where we’ll clean and refine it for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3b30d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define column names based on the provided sample\n",
    "all_columns = ['label', 'claim', 'topics', 'originator', 'title', 'party', 'justification']\n",
    "text_columns = ['claim', 'originator', 'title', 'party', 'justification']\n",
    "\n",
    "# Load the JSONL datasets\n",
    "def load_jsonl(file_path, columns):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "train_df = load_jsonl('train2.jsonl', all_columns)\n",
    "test_df = load_jsonl('test2.jsonl', all_columns)\n",
    "val_df = load_jsonl('val2.jsonl', all_columns)\n",
    "\n",
    "# Show a brief summary of the dataset\n",
    "display(train_df.info())\n",
    "display(train_df.describe())\n",
    "display(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c75fd",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "After loading the dataset, we conduct an exploratory data analysis (EDA) to better understand its structure and content. This step is crucial for identifying any potential issues and gaining insights that will guide the subsequent stages of model development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9838b25e",
   "metadata": {},
   "source": [
    "#### Visualizing Label Distribution\n",
    "\n",
    "To further understand the dataset, we visualize the distribution of truthfulness labels within the training data. This analysis is important for detecting class imbalances, which can impact the performance of the machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de67764",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "label_distribution = train_df['label'].value_counts().reset_index()\n",
    "label_distribution.columns = ['label', 'count']\n",
    "\n",
    "# Create the interactive pie chart\n",
    "fig = px.pie(label_distribution, values='count', names='label', title='Distribution of Truthfulness Labels',\n",
    "             color_discrete_sequence=px.colors.qualitative.Pastel)\n",
    "\n",
    "# Update the layout to make the hover more informative\n",
    "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "\n",
    "# Show the interactive chart\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ceeb24",
   "metadata": {},
   "source": [
    "#### Class Imbalance Detection\n",
    "\n",
    "Given that class imbalance can heavily influence model performance, we calculate the class weights to understand the imbalance better. This information is critical for deciding whether to apply techniques like class weighting, oversampling, or undersampling during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n",
    "class_weights_dict = dict(zip(np.unique(train_df['label']), class_weights))\n",
    "\n",
    "display(pd.DataFrame(list(class_weights_dict.items()), columns=['Label', 'Class Weight']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5efd8e1",
   "metadata": {},
   "source": [
    "#### Text Length Distribution\n",
    "\n",
    "Analyzing the distribution of claim lengths helps us determine whether the length of a claim influences its classification. This analysis can also highlight outliers that may require special attention during preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2101b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the length of each claim\n",
    "train_df['claim_length'] = train_df['claim'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Plot the distribution of claim lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_df['claim_length'], bins=np.linspace(train_df['claim_length'].min(), train_df['claim_length'].max(), 40), color='lightcoral', edgecolor='black')\n",
    "plt.title('Distribution of Claim Lengths')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b88bd",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Ensuring data integrity is crucial before proceeding with text preprocessing and feature extraction. In this step, we clean the dataset by removing any rows with missing (`NaN`) values or empty strings. These problematic entries can introduce noise or bias, which could negatively impact model performance.\n",
    "\n",
    "We implement a `filter_problematic_rows` function to systematically remove these entries across the training, testing, and validation datasets to maintain consistency. Once the cleaning process is complete, we inspect the cleaned datasets to verify the removal of problematic rows and ensure that the data is ready for subsequent preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter out rows with NaN values or empty string values\n",
    "def filter_problematic_rows(df):\n",
    "    return df.dropna().loc[~df.applymap(lambda x: x == '' if isinstance(x, str) else False).any(axis=1)]\n",
    "\n",
    "# Filter out problematic rows in all DataFrames\n",
    "train_df = filter_problematic_rows(train_df)\n",
    "test_df = filter_problematic_rows(test_df)\n",
    "val_df = filter_problematic_rows(val_df)\n",
    "\n",
    "# Display the cleaned datasets\n",
    "display(train_df.info())\n",
    "display(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa1e2b",
   "metadata": {},
   "source": [
    "After cleaning, the training dataset now contains 6,795 entries, with all columns free of null values, confirming data integrity.\n",
    "\n",
    "\n",
    "### Text Preprocessing\n",
    "\n",
    "To ensure consistency and remove noise, we preprocess the text data by applying normalization, tokenization, lemmatization, and stop word removal. This standardizes the text data, improving feature extraction and model performance. We apply this preprocessing pipeline to relevant text columns across the training, testing, and validation datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65359868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Normalization\n",
    "    text = str(text).lower()\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z\\s-]', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Stop Words Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in lemmas if word not in stop_words]\n",
    "\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "\n",
    "# Apply preprocessing to specified text columns\n",
    "def apply_text_preprocessing(df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "# Preprocess each text column\n",
    "train_df = apply_text_preprocessing(train_df, text_columns)\n",
    "test_df = apply_text_preprocessing(test_df, text_columns)\n",
    "val_df = apply_text_preprocessing(val_df, text_columns)\n",
    "\n",
    "display(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0084e07",
   "metadata": {},
   "source": [
    "After preprocessing, we display the first few entries of the training dataset to verify that the text has been cleaned and standardized. This step is crucial for ensuring the data is ready for feature extraction and subsequent model training, reducing model complexity and improving accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe694a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Calculating Credibility Scores\n",
    "\n",
    "To complement the textual features, we calculate speaker credibility scores based on the historical truthfulness of their statements. This step enhances the model’s ability to factor in the reliability of the information source, aligning with our objective of improving both accuracy and interpretability. The scores are mapped to labels on a scale from 0 (least truthful) to 1.0 (most truthful), reflecting the degree of truthfulness associated with each label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d41fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the label to score mapping\n",
    "label_to_score = {\n",
    "    'pants-fire': 0,\n",
    "    'false': 0.2,\n",
    "    'barely-true': 0.4,\n",
    "    'half-true': 0.6,\n",
    "    'mostly-true': 0.8,\n",
    "    'true': 1.0\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_credibility(df, label_to_score):\n",
    "    df['label_score'] = df['label'].map(label_to_score)\n",
    "    credibility_scores = df.groupby('originator')['label_score'].mean()\n",
    "    return credibility_scores\n",
    "\n",
    "\n",
    "# Calculate the speaker credibility scores for each dataset individually\n",
    "train_credibility_scores = calculate_credibility(train_df, label_to_score)\n",
    "test_credibility_scores = calculate_credibility(test_df, label_to_score)\n",
    "val_credibility_scores = calculate_credibility(val_df, label_to_score)\n",
    "\n",
    "\n",
    "# Map the scores back to each dataset individually\n",
    "train_df['credibility_score'] = train_df['originator'].map(train_credibility_scores)\n",
    "test_df['credibility_score'] = test_df['originator'].map(test_credibility_scores)\n",
    "val_df['credibility_score'] = val_df['originator'].map(val_credibility_scores)\n",
    "\n",
    "\n",
    "# Display the updated train_df\n",
    "display(train_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaccaaa",
   "metadata": {},
   "source": [
    "### Visualizing Credibility Scores\n",
    "\n",
    "To better understand the distribution of the calculated credibility scores, we plot a histogram. This visualization helps to assess how the credibility scores are distributed across the dataset, which is important for understanding how these scores might influence the model’s predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6debf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the distribution of credibility scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_df['credibility_score'], bins=np.linspace(0, 1, 30), color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Credibility Scores')\n",
    "plt.xlabel('Credibility Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579deb3",
   "metadata": {},
   "source": [
    "**The following observations can be made based on the distribution of credibility scores:**\n",
    "\n",
    "- **Central Tendency —** The distribution peaks between 0.5 and 0.6, where most originators have credibility scores, indicating moderate truthfulness.\n",
    "\n",
    "- **Distribution Spread —** Although the distribution spans nearly all bins, there is a pronounced concentration between 0.5 and 0.6, suggesting a balanced mix of truthfulness and falsity.\n",
    "\n",
    "- **Low and High Extremes —** Distinct clusters at both low and high ends represent consistently unreliable or highly credible originators, highlighting variability in source reliability.\n",
    "\n",
    "- **Gaps and Low Frequencies —** Certain bins have low counts or gaps, suggesting fewer originators with scores in these intervals, indicating distinct groupings in the data.\n",
    "\n",
    "The balanced spread of credibility scores, with a central tendency, enhances the model's ability to identify subtle differences in source reliability, potentially improving accuracy and interpretability. Additionally, the presence of outliers provides an opportunity for the model to learn from extreme cases, further refining its predictive power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ce9a3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3. Data Subset for Prototyping\n",
    "\n",
    "Before moving to model training, a subset of the data is selected for initial experiments. This allows for quicker iterations and testing of the modeling pipeline, ensuring everything functions correctly before scaling up to the full dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f44efa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select a subset of the data\n",
    "subset_size = 500\n",
    "\n",
    "X_train_subset = train_df[:subset_size].reset_index(drop=True)\n",
    "y_train_subset = train_df['label'][:subset_size].reset_index(drop=True)\n",
    "\n",
    "X_val_subset = val_df[:subset_size].reset_index(drop=True)\n",
    "y_val_subset = val_df['label'][:subset_size].reset_index(drop=True)\n",
    "\n",
    "X_test_subset = test_df[:subset_size].reset_index(drop=True)\n",
    "y_test_subset = test_df['label'][:subset_size].reset_index(drop=True)\n",
    "\n",
    "display(X_train_subset)\n",
    "display(y_train_subset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa918991",
   "metadata": {},
   "source": [
    "**Subsetting Strategy:**\n",
    "\n",
    "- **Computational Efficiency —** A subset size of 500 is selected to balance the need for quick iteration while having enough data points to yield meaningful insights during initial testing.\n",
    "\n",
    "- **Index Reset —** The `reset_index(drop=True)` method is applied to ensure that indices in the subsets start at 0. This prevents misalignment issues during data processing, which is crucial for maintaining consistency across different stages of the modeling pipeline.\n",
    "\n",
    "\n",
    "### Combining Preprocessed Text Columns\n",
    "\n",
    "To prepare our data for vectorization and subsequent model training, we consolidate all preprocessed text columns into a single text column for each dataset entry. This step simplifies the input format for our models, allowing them to process each text entry as a single data point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59678838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text_columns(df, columns):\n",
    "    return df.apply(lambda row: ' '.join([str(val) for val in row[columns]]), axis=1)\n",
    "\n",
    "# Combine the text columns for each row ensuring all values are converted to strings\n",
    "X_train_textual_features = combine_text_columns(X_train_subset, text_columns)\n",
    "X_test_textual_features = combine_text_columns(X_test_subset, text_columns)\n",
    "X_val_textual_features = combine_text_columns(X_val_subset, text_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a2013",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Extraction & Engineering\n",
    "\n",
    "### TF-IDF Vectorization\n",
    "\n",
    "The following code applies TF-IDF vectorization to the preprocessed textual features, converting the text data into numerical representations with a maximum of 1000 features. The vectorizer is first fit on the training data and then used to transform the validation and test sets, ensuring consistent feature extraction across datasets. Finally, the shapes of the resulting matrices are printed to confirm that the vectorization has been applied correctly, with each matrix representing the top 1000 terms across all samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "max_features_tfidf = 1000\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features_tfidf)\n",
    "\n",
    "# Fit and transform the vectorizer on data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_textual_features)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_textual_features)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val_textual_features)\n",
    "\n",
    "# Print the shape of the resulting matrices\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)\n",
    "print(X_val_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0155a24",
   "metadata": {},
   "source": [
    "### Feature Name Extraction\n",
    "\n",
    "The TF-IDF feature names from the training subset are retrieved here. Printing the first 10 feature names offers an early glimpse into the key terms that the model will consider, which is essential for later stages where these names are used in model interpretability, such as in SHAP visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4483e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature and class names\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_feature_names_with_scores = np.append(tfidf_feature_names, 'credibility_score')\n",
    "\n",
    "# Print top 10 features\n",
    "print(\"Feature names: \", tfidf_feature_names[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8cd44",
   "metadata": {},
   "source": [
    "### TF-IDF Sparse Matrix Conversion\n",
    "\n",
    "This section integrates the credibility scores with the TF-IDF features, enhancing the model’s ability to factor in source reliability. The credibility scores are reshaped and combined with the dense TF-IDF matrices, which are then converted back to a sparse format for computational efficiency. The matrix shapes are checked to confirm the correct addition of this new feature, which is expected to improve model performance by leveraging historical trustworthiness in predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b05a88a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "credibility_score_train = X_train_subset['credibility_score'].values.reshape(-1, 1)\n",
    "credibility_score_test = X_test_subset['credibility_score'].values.reshape(-1, 1)\n",
    "credibility_score_val = X_val_subset['credibility_score'].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Combine the dense TF-IDF matrices with the originator scores\n",
    "X_train_dense_with_scores = X_train_dense_with_scores if 'X_train_dense_with_scores' in locals() else np.hstack((X_train_tfidf.toarray(), credibility_score_train))\n",
    "X_test_dense_with_scores = X_test_dense_with_scores if 'X_test_dense_with_scores' in locals() else np.hstack((X_test_tfidf.toarray(), credibility_score_test))\n",
    "X_val_dense_with_scores = X_val_dense_with_scores if 'X_val_dense_with_scores' in locals() else np.hstack((X_val_tfidf.toarray(), credibility_score_val))\n",
    "\n",
    "\n",
    "# Convert the combined dense matrices back to sparse format\n",
    "X_train_tfidf_with_scores = csr_matrix(X_train_dense_with_scores)\n",
    "X_test_tfidf_with_scores = csr_matrix(X_test_dense_with_scores)\n",
    "X_val_tfidf_with_scores = csr_matrix(X_val_dense_with_scores)\n",
    "\n",
    "\n",
    "# Convert the sparse TF-IDF matrices to dense format\n",
    "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
    "X_test_tfidf_dense = X_test_tfidf.toarray()\n",
    "X_val_tfidf_dense = X_val_tfidf.toarray()\n",
    "\n",
    "X_train_tfidf_dense_with_scores = X_train_tfidf_with_scores.toarray()\n",
    "X_test_tfidf_dense_with_scores = X_test_tfidf_with_scores.toarray()\n",
    "X_val_tfidf_dense_with_scores = X_val_tfidf_with_scores.toarray()\n",
    "\n",
    "\n",
    "# Print the shape of the resulting matrices\n",
    "print(X_train_tfidf.shape)  # Expected output: (500, 1000)\n",
    "print(X_test_tfidf.shape)   # Expected output: (500, 1000)\n",
    "print(X_val_tfidf.shape)    # Expected output: (500, 1000)\n",
    "\n",
    "print()\n",
    "\n",
    "print(X_train_tfidf_with_scores.shape)  # Expected output: (500, 1001)\n",
    "print(X_test_tfidf_with_scores.shape)   # Expected output: (500, 1001)\n",
    "print(X_val_tfidf_with_scores.shape)    # Expected output: (500, 1001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc33928",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Model Development and Evaluation\n",
    "\n",
    "### Model Training\n",
    "\n",
    "In this step, we initialize and train two Random Forest models to assess the impact of integrating credibility scores on fake news detection:\n",
    "\n",
    "- **Baseline Model —** Trained using only TF-IDF features, this model serves as a reference for traditional text-based classification.\n",
    "\n",
    "- **Enhanced Model —** This model incorporates both TF-IDF features and credibility scores, allowing it to account for the historical reliability of the information source.\n",
    "\n",
    "By comparing these models, we aim to determine if the addition of credibility scores improves predictive accuracy and provides more context-aware decisions, which is critical for enhancing both the performance and interpretability of fake news detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f202b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier_with_scores = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train_tfidf, y_train_subset)\n",
    "rf_classifier_with_scores.fit(X_train_tfidf_with_scores, y_train_subset)\n",
    "\n",
    "# Get the list of unique labels\n",
    "classes = rf_classifier.classes_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af17ed26",
   "metadata": {},
   "source": [
    "### Class-Specific Evaluation\n",
    "\n",
    "#### Evaluating Model Performance with Validation Data\n",
    "\n",
    "After training both models on the training data, predictions are made on the validation set to assess the models' performance. This step is critical for understanding how well the models generalize to new, unseen data, highlighting the effect of adding credibility scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358689ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def generate_predictions(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def generate_classification_df(y_true, y_pred, model_label=None):\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # If model_label is provided, rename columns; otherwise, keep original metric names\n",
    "    if model_label:\n",
    "        df.columns = [f\"{col}_{model_label}\" for col in df.columns]\n",
    "    \n",
    "    return df\n",
    "\n",
    "y_val_pred = generate_predictions(rf_classifier, X_val_tfidf)\n",
    "y_val_pred_with_scores = generate_predictions(rf_classifier_with_scores, X_val_tfidf_with_scores)\n",
    "\n",
    "df_without_scores = generate_classification_df(y_val_subset, y_val_pred)  # No label for 'without scores'\n",
    "df_with_scores = generate_classification_df(y_val_subset, y_val_pred_with_scores, 'with_scores')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9626c77c",
   "metadata": {},
   "source": [
    "#### Comparing Performance Metrics Between Models\n",
    "\n",
    "By comparing key metrics like precision, recall, and F1-score between the baseline and enhanced models, we measure the impact of adding credibility scores. This comparison helps us understand how much the inclusion of these scores improves or changes model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage_change(df1, df2):\n",
    "    # Combine the DataFrames side by side\n",
    "    comparison_df = pd.concat([df1, df2], axis=1)\n",
    "    \n",
    "    # Calculate the percentage change for precision, recall, and f1-score\n",
    "    for metric in ['precision', 'recall', 'f1-score']:\n",
    "        comparison_df[f'{metric}_change'] = (\n",
    "            (comparison_df[f'{metric}_with_scores'] - comparison_df[metric]) /\n",
    "            comparison_df[metric]\n",
    "        ) * 100\n",
    "    \n",
    "    # Reorder the columns for side-by-side comparison with percentage changes\n",
    "    column_order = []\n",
    "    for metric in ['precision', 'recall', 'f1-score']:\n",
    "        column_order.extend([metric, f'{metric}_with_scores', f'{metric}_change'])\n",
    "    \n",
    "    comparison_df = comparison_df[column_order]\n",
    "    \n",
    "    # Drop the 'accuracy' row\n",
    "    comparison_df = comparison_df.drop('accuracy', axis=0)\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "comparison_df = calculate_percentage_change(df_without_scores, df_with_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feebf63",
   "metadata": {},
   "source": [
    "#### Analyzing Model Performance Metrics\n",
    "\n",
    "This step brings together the performance metrics comparison, emphasizing the impact of adding credibility scores. By organizing and clearly highlighting changes, we aim to provide a comprehensive understanding of how these scores influence model performance, making the differences between the baseline and enhanced models more apparent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14487c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_comparison_df(df):\n",
    "    def format_percentage(val):\n",
    "        return f\"{val:.2f}%\" if pd.notna(val) else \"\"\n",
    "\n",
    "    def highlight_change(val):\n",
    "        color = 'lightgreen' if val > 0 else 'lightcoral' if val < 0 else ''\n",
    "        return f'background-color: {color}'\n",
    "\n",
    "    def add_border(s):\n",
    "        styles = ['border-right: 1px solid black' if i % 3 == 2 else '' for i in range(len(s))]\n",
    "        if s.name == 'true': styles = [style + '; border-bottom: 1px solid black' for style in styles]\n",
    "        return styles\n",
    "\n",
    "    return (\n",
    "        df.style\n",
    "        .applymap(highlight_change, subset=[f'{metric}_change' for metric in ['precision', 'recall', 'f1-score']])\n",
    "        .apply(add_border, axis=1)\n",
    "        .format(precision=2)\n",
    "        .format(formatter=format_percentage, subset=[f'{metric}_change' for metric in ['precision', 'recall', 'f1-score']])\n",
    "    )\n",
    "\n",
    "styled_comparison_df = style_comparison_df(comparison_df)\n",
    "\n",
    "styled_comparison_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d873b",
   "metadata": {},
   "source": [
    "Incorporating credibility scores led to substantial improvements across key metrics:\n",
    "\n",
    "- **Significant Precision Gains:**\n",
    "\n",
    "    - True: Precision improved by 102.76%, indicating better identification of true statements.\n",
    "    - Barely-True, False, Mostly-True: Precision increased by 45.83%, 49.72%, and 64.35%, respectively, showing enhanced distinction between truthfulness levels.\n",
    "\n",
    "\n",
    "- **Key Recall Improvement:**\n",
    "\n",
    "    - False: Recall doubled (100% increase), making the model more sensitive to false statements.\n",
    "    - True: Recall increased by 140%, reducing missed true statements.\n",
    "\n",
    "- **F1-Score Enhancement:**\n",
    "\n",
    "    - Most categories saw F1-score improvements, especially \"True,\" which increased by 123.64%.\n",
    "    - Overall Gains: Both macro and weighted F1-scores improved substantially by 50.27% and 47.68%.\n",
    "\n",
    "Integrating credibility scores has significantly enhanced the model’s precision, recall, and F1-scores, especially in detecting true and false statements. However, challenges remain in the \"Pants-Fire\" category, likely due to data scarcity or the subtlety of these cases, suggesting the need for further refinement. Overall, credibility scores have made the model more accurate and reliable across diverse truthfulness levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5a259",
   "metadata": {},
   "source": [
    "### Overall Metrics Evaluation\n",
    "\n",
    "#### Evaluating Overall Model Metrics\n",
    "\n",
    "To assess the broader impact of integrating credibility scores, we calculate key overall metrics for both the baseline and enhanced models. These metrics include accuracy, weighted precision, recall, F1-score, ROC-AUC score, log loss, and mean cross-validation accuracy. By comparing these metrics, we gain a comprehensive understanding of how credibility scores influence overall model performance, complementing our earlier class-specific analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a19e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Calculate overall metrics for the baseline model\n",
    "metrics_without = {\n",
    "    'Accuracy': accuracy_score(y_val_subset, y_val_pred),\n",
    "    'Precision': precision_score(y_val_subset, y_val_pred, average='weighted', zero_division=0),\n",
    "    'Recall': recall_score(y_val_subset, y_val_pred, average='weighted'),\n",
    "    'F1-Score': f1_score(y_val_subset, y_val_pred, average='weighted'),\n",
    "    'ROC-AUC Score': roc_auc_score(y_val_subset, rf_classifier.predict_proba(X_val_tfidf), multi_class='ovr'),\n",
    "    'Log Loss': log_loss(y_val_subset, rf_classifier.predict_proba(X_val_tfidf)),\n",
    "    'Mean Cross-Validation Accuracy': cross_val_score(rf_classifier, X_train_tfidf, y_train_subset, cv=5, scoring='accuracy').mean()\n",
    "}\n",
    "\n",
    "# Calculate overall metrics for the enhanced model (with credibility scores)\n",
    "metrics_with = {\n",
    "    'Accuracy': accuracy_score(y_val_subset, y_val_pred_with_scores),\n",
    "    'Precision': precision_score(y_val_subset, y_val_pred_with_scores, average='weighted', zero_division=0),\n",
    "    'Recall': recall_score(y_val_subset, y_val_pred_with_scores, average='weighted'),\n",
    "    'F1-Score': f1_score(y_val_subset, y_val_pred_with_scores, average='weighted'),\n",
    "    'ROC-AUC Score': roc_auc_score(y_val_subset, rf_classifier_with_scores.predict_proba(X_val_tfidf_with_scores), multi_class='ovr'),\n",
    "    'Log Loss': log_loss(y_val_subset, rf_classifier_with_scores.predict_proba(X_val_tfidf_with_scores)),\n",
    "    'Mean Cross-Validation Accuracy': cross_val_score(rf_classifier_with_scores, X_train_tfidf_with_scores, y_train_subset, cv=5, scoring='accuracy').mean()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dfb0d4",
   "metadata": {},
   "source": [
    "#### Comparing Overall Model Metrics\n",
    "\n",
    "Next, we consolidate the calculated metrics into a single DataFrame to facilitate a clear comparison between the baseline and enhanced models. Additionally, we compute the percentage change to quantify the impact of adding credibility scores on overall performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0034742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the metrics side by side\n",
    "metrics_comparison_df = pd.DataFrame({\n",
    "    'Metric': metrics_without.keys(),\n",
    "    'Without Scores': metrics_without.values(),\n",
    "    'With Scores': metrics_with.values()\n",
    "})\n",
    "\n",
    "# Calculate the percentage change\n",
    "metrics_comparison_df['Change (%)'] = ((metrics_comparison_df['With Scores'] - metrics_comparison_df['Without Scores']) / metrics_comparison_df['Without Scores']) * 100\n",
    "\n",
    "# Round the values for readability\n",
    "metrics_comparison_df = metrics_comparison_df.round(4)\n",
    "\n",
    "# Function to format percentage change\n",
    "def format_percentage(val):\n",
    "    return f\"{val:.2f}%\" if pd.notna(val) else \"\"\n",
    "\n",
    "# Apply the percentage formatting\n",
    "metrics_comparison_df['Change (%)'] = metrics_comparison_df['Change (%)'].apply(format_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26638096",
   "metadata": {},
   "source": [
    "#### Interpreting the Impact of Credibility Scores\n",
    "\n",
    "This evaluation compares the baseline and enhanced models across key metrics to assess the impact of integrating credibility scores. The \"Change (%)\" column quantifies these differences, highlighting improvements or declines. This comparison is crucial for determining whether credibility scores consistently enhance model accuracy and interpretability, directly supporting the project’s core objectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91eb603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply conditional styling to the \"Change (%)\" column\n",
    "styled_metrics_comparison_df = metrics_comparison_df.style.applymap(\n",
    "    lambda x: 'background-color: lightgreen' if isinstance(x, str) and x.endswith('%') and float(x[:-1]) > 0 else \n",
    "              'background-color: lightcoral' if isinstance(x, str) and x.endswith('%') and float(x[:-1]) < 0 else '',\n",
    "    subset=['Change (%)']\n",
    ")\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_metrics_comparison_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfeb773",
   "metadata": {},
   "source": [
    "The comparison between the baseline and enhanced models shows significant improvements across key metrics with the integration of credibility scores. Accuracy and recall both increased by 44.85%, precision by 49.99%, and the F1-score by 47.68%. The ROC-AUC score also saw a notable rise of 26.73%.\n",
    "\n",
    "Additionally, log loss decreased by 18.93%, indicating better model confidence, while mean cross-validation accuracy improved by 23.91%. These results confirm that credibility scores substantially enhance the model’s overall performance, aligning with the project’s goal of improving both accuracy and interpretability in fake news detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7fa30",
   "metadata": {},
   "source": [
    "### Precision-Recall Curves for Multiclass Classification\n",
    "\n",
    "To further evaluate model performance, we generate and compare precision-recall curves for a multiclass classification models, highlighting the impact of incorporating credibility scores on model performance. The left plot shows precision-recall curves for each class using the baseline model without credibility scores, while the right plot displays the same curves after adding credibility scores. By visualizing these side-by-side, we can assess how the integration of credibility scores enhances the model's ability to balance precision and recall across different classes, thereby improving overall accuracy in fake news detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08556d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Binarize the labels for the precision-recall curve\n",
    "y_val_bin = label_binarize(y_val_subset, classes=classes)\n",
    "\n",
    "# Initialize the plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Plot Precision-Recall curves for each class without credibility scores\n",
    "axes[0].set_title('Precision-Recall Curve Without Credibility Scores')\n",
    "for i, class_label in enumerate(classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_val_bin[:, i], rf_classifier.predict_proba(X_val_tfidf)[:, i])\n",
    "    axes[0].plot(recall, precision, lw=2, label=f'Precision-Recall for {class_label}')\n",
    "axes[0].set_xlabel('Recall')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid()\n",
    "\n",
    "# Plot Precision-Recall curves for each class with credibility scores\n",
    "axes[1].set_title('Precision-Recall Curve With Credibility Scores')\n",
    "for i, class_label in enumerate(classes):\n",
    "    precision, recall, _ = precision_recall_curve(y_val_bin[:, i], rf_classifier_with_scores.predict_proba(X_val_tfidf_with_scores)[:, i])\n",
    "    axes[1].plot(recall, precision, lw=2, label=f'Precision-Recall for {class_label}')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].legend(loc='best')\n",
    "axes[1].grid()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27eb77e",
   "metadata": {},
   "source": [
    "The integration of credibility scores leads to a notable improvement in the model’s precision across all classes, particularly at higher recall levels. This enhancement demonstrates the effectiveness of credibility scores in refining the model's predictions, reducing false positives, and boosting confidence in correctly classified instances. These results strongly support the conclusion that incorporating credibility scores not only enhances the accuracy of fake news detection but also contributes to more reliable and context-aware decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b520c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Confusion Matrix Comparison\n",
    "\n",
    "This step presents a detailed comparison of the models' performance using confusion matrices. The first two heatmaps display the classification results for the baseline and enhanced models, respectively. The third heatmap shows the difference between these matrices, highlighting the specific areas where the model's performance improved or declined with the integration of credibility scores. Positive values in the difference matrix indicate improvements, while negative values suggest declines. This visualization offers a clear perspective on how credibility scores influence the model’s classification accuracy across different classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd5d20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate confusion matrices\n",
    "conf_matrix_without = confusion_matrix(y_val_subset, y_val_pred, labels=classes)\n",
    "conf_matrix_with = confusion_matrix(y_val_subset, y_val_pred_with_scores, labels=classes)\n",
    "\n",
    "# Calculate the difference matrix\n",
    "diff_matrix = conf_matrix_with - conf_matrix_without\n",
    "\n",
    "# Display the confusion matrices as heatmaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.heatmap(conf_matrix_without, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0])\n",
    "axes[0].set_title(\"Without Credibility Scores\")\n",
    "axes[0].set_xticklabels(classes, rotation=45, ha=\"right\")\n",
    "axes[0].set_yticklabels(classes, rotation=0)  # Set y labels horizontally\n",
    "\n",
    "sns.heatmap(conf_matrix_with, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[1])\n",
    "axes[1].set_title(\"With Credibility Scores\")\n",
    "axes[1].set_xticklabels(classes, rotation=45, ha=\"right\")\n",
    "axes[1].set_yticklabels(classes, rotation=0)  # Set y labels horizontally\n",
    "\n",
    "sns.heatmap(diff_matrix, annot=True, fmt=\"d\", cmap=\"RdBu\", center=0, ax=axes[2])\n",
    "axes[2].set_title(\"Difference (With - Without Scores)\")\n",
    "axes[2].set_xticklabels(classes, rotation=45, ha=\"right\")\n",
    "axes[2].set_yticklabels(classes, rotation=0)  # Set y labels horizontally\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d8174",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Model Interpretability\n",
    "\n",
    "### Using SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "Having developed and evaluated our models, the final step focuses on enhancing interpretability. We employ SHAP (SHapley Additive exPlanations) to provide clear insights into the factors driving model predictions. This approach not only helps validate model behavior but also aligns with our objective of ensuring transparency alongside accuracy.\n",
    "\n",
    "In this section, we initialize SHAP explainers for both the baseline and enhanced models. We then compute SHAP values, which will be used to visualize and interpret the impact of individual features on our predictions, including the credibility scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Initialize JavaScript visualization in Jupyter Notebook\n",
    "shap.initjs()\n",
    "\n",
    "# Initialize the SHAP explainer with dense training data\n",
    "rf_explainer = shap.TreeExplainer(rf_classifier, X_train_tfidf_dense)\n",
    "rf_explainer_with_scores = shap.TreeExplainer(rf_classifier_with_scores, X_train_tfidf_dense_with_scores)\n",
    "\n",
    "# Generate SHAP values for the validation data\n",
    "rf_shap_values = rf_explainer.shap_values(X_train_tfidf_dense)\n",
    "rf_shap_values_with_scores = rf_explainer_with_scores.shap_values(X_train_tfidf_dense_with_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e75480d",
   "metadata": {},
   "source": [
    "### SHAP Analysis for Model Interpretability\n",
    "\n",
    "After initializing SHAP explainers and computing SHAP values, we now turn to the detailed analysis of individual sample predictions. In this section, we apply SHAP to a subset of validation samples to compare the baseline model and the enhanced model with credibility scores. This comparison highlights how different features contribute to each prediction and allows us to visually assess the impact of incorporating credibility scores.\n",
    "\n",
    "This step iterates through a selected set of samples, displaying relevant data, model predictions, actual labels, and SHAP force plots. This process provides a clear, interpretable view of how the models arrive at their decisions, reinforcing the transparency and explainability that are central to our approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad5a2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Initialize SHAP visualization\n",
    "shap.initjs()\n",
    "\n",
    "# Define the number of samples to analyze\n",
    "rf_num_samples = 10\n",
    "rf_sample_indices = range(rf_num_samples)\n",
    "\n",
    "# Generate predictions and SHAP values for both models\n",
    "rf_sample_predictions = rf_classifier.predict(X_val_tfidf[rf_sample_indices])\n",
    "rf_sample_shap_values = rf_explainer.shap_values(X_val_tfidf_dense[rf_sample_indices])\n",
    "rf_sample_predictions_with_scores = rf_classifier_with_scores.predict(X_val_tfidf_with_scores[rf_sample_indices])\n",
    "rf_sample_shap_values_with_scores = rf_explainer_with_scores.shap_values(X_val_tfidf_dense_with_scores[rf_sample_indices])\n",
    "\n",
    "# Function to display results and SHAP plots\n",
    "def display_sample_info(index, prediction, shap_values, shap_explainer, data, feature_names, include_all_columns=True):\n",
    "    row = X_val_subset.iloc[index]\n",
    "    columns = list(row.index) + ['Prediction', 'Actual Label']\n",
    "    values = list(row.values) + [prediction, y_val_subset.iloc[index]]\n",
    "\n",
    "    sample_df = pd.DataFrame([values], columns=columns)\n",
    "\n",
    "    # Highlight predictions and actual labels\n",
    "    styled_df = sample_df.style.applymap(lambda x: 'background-color: #ffff99' if x in [prediction, y_val_subset.iloc[index]] else '', subset=['Prediction', 'Actual Label']).hide(axis=\"index\")\n",
    "    display(styled_df)\n",
    "\n",
    "    # SHAP Force Plot\n",
    "    display(shap.force_plot(shap_explainer.expected_value[1], shap_values[1][index], data[index], feature_names=feature_names))\n",
    "\n",
    "# Iterate over sample indices and display information\n",
    "for i in rf_sample_indices:\n",
    "    display_sample_info(i, rf_sample_predictions[i], rf_sample_shap_values, rf_explainer, X_val_tfidf_dense, tfidf_feature_names)\n",
    "    display_sample_info(i, rf_sample_predictions_with_scores[i], rf_sample_shap_values_with_scores, rf_explainer_with_scores, X_val_tfidf_dense_with_scores, tfidf_feature_names_with_scores)\n",
    "\n",
    "    # Separator for readability between samples\n",
    "    display(HTML(\"<div style='text-align: center; font-weight: bold; font-size: 20px;'>{}<div>\".format(\"=\"*80)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a87527",
   "metadata": {},
   "source": [
    "### Comparing Feature Importance with SHAP Summary Plots\n",
    "\n",
    "In this section, we visualize the feature importance for both the baseline model (using only TF-IDF features) and the enhanced model (with added credibility scores). By generating separate SHAP summary plots for each model, we can clearly observe which features contribute most to the predictions in each case. The bar plot format provides an intuitive view of the average impact of each feature across all classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6977a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(rf_shap_values, X_val_tfidf_dense, plot_type='bar', feature_names=tfidf_feature_names, class_names=classes)\n",
    "shap.summary_plot(rf_shap_values_with_scores, X_val_tfidf_dense_with_scores, plot_type='bar', feature_names=tfidf_feature_names_with_scores, class_names=classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e6865",
   "metadata": {},
   "source": [
    "The integration of credibility scores into the fake news detection model has led to a substantial improvement in both accuracy and interpretability. The SHAP analysis highlights a significant shift in feature importance, with the `credibility_score` emerging as the most influential factor. This shift indicates that the enhanced model is more contextually aware, using the historical reliability of sources to make more informed predictions.\n",
    "\n",
    "While the credibility score now dominates, the model still effectively incorporates key textual features, ensuring that content remains a vital part of the decision-making process. This balanced approach reflects the success of the implementation, demonstrating that combining source credibility with traditional text analysis creates a more robust and interpretable model. Overall, the results strongly support the effectiveness of the methodology in improving fake news detection.\n",
    "\n",
    "That said, the dominance of the credibility score raises an important consideration. While this feature greatly enhances context-awareness, it could lead to an over-reliance on the source’s historical behavior, potentially overshadowing the content’s actual value. Ensuring a balanced influence between credibility and content-specific features will be crucial for maintaining the model’s robustness across varied scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b2b47",
   "metadata": {},
   "source": [
    "### Feature Interaction Analysis with SHAP Dependence Plots\n",
    "\n",
    "In this section, we generate SHAP dependence plots to analyze how a specific feature (e.g. 'obama'), influences predictions across different classes. The process involves iterating through each class and creating a plot that shows the relationship between this feature and the model's SHAP values. This analysis helps us understand how the presence of 'obama' impacts the model's decision-making, while also considering its interaction with another feature, 'care'.\n",
    "\n",
    "Each class's dependence plot provides visual insight into the model's behavior, highlighting how different classes respond to the feature. By examining these plots, we gain a deeper understanding of the model's inner workings, ensuring that predictions are both accurate and interpretable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b11aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example feature name for the dependence plot\n",
    "feature_name = 'obama'\n",
    "\n",
    "# Loop through each class to create dependence plots\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    print(f\"Dependence plot for class: {class_name}\")\n",
    "    shap.dependence_plot(feature_name, rf_shap_values_with_scores[class_idx], X_val_tfidf_dense_with_scores, feature_names=tfidf_feature_names_with_scores, interaction_index=\"care\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7404bc19",
   "metadata": {},
   "source": [
    "### Generating SHAP Summary Plots for Each Class\n",
    "\n",
    "To further delve into the specifics of how different features influence the predictions of various truthfulness categories in our model, we employ SHAP summary plots. These visualizations are created for each class to display the aggregate importance of each feature and the distribution of their effects. Below we iteratively generate both bar and dot SHAP summary plots for each truthfulness category:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e382bebc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the SHAP summary dot plot for each class\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    print(f\"Summary Bar Plot for class: {class_name}\")\n",
    "    shap.summary_plot(rf_shap_values_with_scores[class_idx], X_val_tfidf_dense_with_scores, feature_names=tfidf_feature_names_with_scores, plot_type=\"bar\", class_names=[class_name])\n",
    "\n",
    "    print(f\"Summary plot for class: {class_name}\")\n",
    "    shap.summary_plot(rf_shap_values_with_scores[class_idx], X_val_tfidf_dense_with_scores, feature_names=tfidf_feature_names_with_scores, class_names=classes, plot_type=\"dot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8345d335",
   "metadata": {},
   "source": [
    "**Explanation of SHAP Summary Plots:**\n",
    "\n",
    "- **Bar Plot:** This visualization ranks features by their overall impact on the model's output for a specific class. Each bar represents the mean absolute SHAP value of a feature, providing a clear view of which features are most influential for the class in question.\n",
    "\n",
    "- **Dot Plot:** Unlike the bar plot, the dot plot displays the distribution and impact of SHAP values for each feature on an individual sample basis. It shows both the positive and negative influences of features on the model's prediction, giving insight into the variability and consistency of feature impacts across the dataset.\n",
    "\n",
    "Creating SHAP summary plots for each class allows us to understand the nuanced contributions of features to different categories of predictions. Bar plots provide a straightforward ranking of feature importance, while dot plots offer a detailed look at how each feature's impact varies among individual predictions. These visualizations are essential tools for interpreting complex model behaviors, enhancing transparency, and guiding model refinement to improve performance and fairness.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
